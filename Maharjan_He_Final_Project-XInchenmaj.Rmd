---
title: "Maharjan_He_Final_Project"
author: "Meelisha Maharjan and Xinchen He"
date: "2022-11-24"
output: powerpoint_presentation
---

## Import libaries

```{r}
# install.packages("dataRetrieval")
library(dataRetrieval)
suppressMessages(library(tidyverse))
library(dataRetrieval)
library(dplyr)
library(ggplot2)
```

## Data Collection

- Define a function to scrap the time series of the streamflow (discharge).
```{r}
# Define function to get discharge timeseries
get_discharge_data <- function(site_no, q_code, start, end) {
  
  # Build the url link
  url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
        q_code,
        '=on&format=html&site_no=',
        site_no,
        '&legacy=&re[â€¦]module=sw&period=&begin_date=',
          start,
        '&end_date=', 
        end)
  q_data <- readLines(url)
  
  # Select lines which contains the data
  table_pattern <- '<tr align="center"><td nowrap="nowrap">'
  q_table <- str_subset(q_data, table_pattern)
  
  # Extract dates and flow(ft3/s) from each line
  dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%Y")
  flows = unlist(str_match_all(q_table, "<span>.*</span>"))
  flows = as.numeric(str_replace_all(flows, "</?span>", ""))
  
  # Put dates and flow together as a matrix
  Q_df = data.frame(
    Date = dates,
    Qobs = flows
  )
  return(Q_df)
}
```

- Define a function to scrap the drainge area of the gauge station
```{r}
get_drainage_area = function(site_no){
   # Scrape the drainage area
   url2 <- paste0('https://waterdata.usgs.gov/nwis/inventory/?site_no=',
        site_no,
        '&agency_cd=USGS')
   d_data <- readLines(url2)
   d_pattern <- "^.*Drainage area"
   d_line <- str_subset(d_data,d_pattern)
   darea <- unique(unlist(str_extract_all(d_line, "[0-9]*\\.?[0-9]*")))[2]
   darea <- as.numeric(darea)
   return(darea)
}

```

- Validate the scraped data with the data provided by the USGS `dataRetrieval`

```{r}
# Specify the inputs: gauge number, q_code is fixed, start date and end date
site_no <- '08324000'
q_code <- '00060'
start <- '2000-01-01'
end <- '2020-12-31'
# Use the function we have to scrape
q_df <- get_discharge_data(site_no, q_code, start, end)

# Get authentic streamflow data using the package
validate_q <- readNWISdv(siteNumber = site_no, parameterCd = q_code,
  start, end)

# Calculate the error between scraped data and the true data
scrape_error = q_df$Qobs - validate_q$X_00060_00003

plot(q_df$Date, scrape_error)
```

- Get streamflow time series and drainge area of the gauges in Massachusetts (TAKE A LOT OF TIME, DO NOT RUN IF YOU HAVE DATA EXISTED IN THE FOLDER!!!)

```{r}
# Load the site list
site_list <- readLines("gauges_Mass.txt")
# create an empty list to save drainage area
da_area <- numeric(length(site_list))#

counter = 1
for (site_no in site_list){
  try({
    da_area[counter] = get_drainage_area(site_no)
    })
  counter = counter + 1
  print(site_no)
}

# Note, not all the sites have valid data in the date range
counter = 1
for (site_no in site_list){
  print(paste('Gauge ',site_no,' Scraping start'))
  
  # scrape discharge data
  q_df <- try({
    get_discharge_data(site_no, q_code, start, end)
  }, silent = TRUE)
  
  # scrape the draiange area
  try({
    da_area[counter] = get_drainage_area(site_no)
    })
  
  # Save the streamflow
  try({
    write.csv(q_df, paste0("streamflows/", site_no,".csv"), row.names = FALSE)
  }, silent = TRUE)
  
  counter = counter + 1
  print(paste('Gauge ',site_no,' Done'))
}

# Save the drainge area
write.csv(data.frame(
  site_no = site_list,
  drainage_area = da_area
), "drainage_area.txt", row.names = FALSE)
```

## Calculate Streamflow Statistics and Visualizations

```{r}
# Note: not all the site have a validate
gauges = read.csv("drainage_area.txt", colClasses = c("character", "numeric"))
```
- Import streamflows
```{r}
# sapply(df, function(x) quantile(x, probs = seq(0, 1, 1/4)))
quantile(x = q_df$Q_obs, probs = c(.05, .5, .95))
```

