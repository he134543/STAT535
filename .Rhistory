url2 <- paste0('https://waterdata.usgs.gov/nwis/inventory/?site_no=',
'01198000',
'&agency_cd=USGS')
d_data <- readLines(url2)
d_pattern <- "^.*Drainage area"
d_line <- str_subset(d_data,d_pattern)
darea <- unique(unlist(str_extract_all(d_line, "[0-9]*\\.?[0-9]*")))[2]
darea
q_table
str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")
URL
url
get_discharge_data <- function(site_no, q_code, start, end) {
# Scrape the streamflow
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
name_exp <- "<tr align=\"center\"><td nowrap=\"nowrap\"> |</span><sup>([A-Z]?)&nbsp;&nbsp;([a-z]?)</sup></td></tr>"
raw_q_data <- str_replace_all(q_table, name_exp, "")
name_exp2 <- " </td><td nowrap=\"nowrap\"><span>"
raw_q_data <- str_replace_all(raw_q_data, name_exp2, " ")
q_data_list <- str_split_fixed(raw_q_data, " ", n = Inf)
q_timeseries_df <- data.frame(q_data_list)
q_timeseries_df <- q_timeseries_df %>%
rename(
'Date' = 'X1',
'Q_obs' = 'X2'
) %>%
mutate(across(c(Q_obs),
as.numeric)) %>%
mutate(Date = as.Date(Date, format = "%m/%d/%Y")
)
return(q_timeseries_df)
}
get_discharge_data("01097000", q_code, start, end)
get_discharge_data("01100500", q_code, start, end)
get_discharge_data("0194500", q_code, start, end)
get_discharge_data("01094500", q_code, start, end)
str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")
unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*"))
q_table
unlist(str_match_all(q_table, "[0-9]*\\.[0-9]"))
# Specify the inputs: gauge number, q_code is fixed, start date and end date
site_no <- '08324000'
q_code <- '00060'
start <- '2000-01-01'
end <- '2020-12-31'
# Define function
get_discharge_data <- function(site_no, q_code, start, end) {
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*\\.[0-9]")))
# Put dates and flow together as a matrix
Q_df = data.frame(
Dates = dates,
Qobs = flows
)
return(Q_df)
}
q_df <- get_discharge_data(site_no, q_code, start, end)
site_no <- '08324000'
q_code <- '00060'
start <- '2000-01-01'
end <- '2020-12-31'
# Define function
get_discharge_data <- function(site_no, q_code, start, end) {
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*\\.[0-9]")))
# Put dates and flow together as a matrix
Q_df = data.frame(
Dates = dates,
Qobs = flows
)
return(Q_df)
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*\\.[0-9]")))
c
cxczx
a
sada)
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*\\.[0-9]")))
flows
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*(\\.?)[0-9]")))
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*(\\.?)[0-9]")))
flows
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*(\\.)?[0-9]")))
flows
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*\\.?[0-9]")))
flows
q_table
head(q_table)
flows = as.numeric(unlist(str_match_all(q_table, "^>[0-9]*\\.?[0-9]")))
flows
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*\\.?[0-9]")))
flows
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = as.numeric(unlist(str_match_all(q_table, "[0-9]*\\.?[0-9]<")))
flows
str_match_all(q_table, "[0-9]*\\.?[0-9]<")
head(q_table)
unlist(str_match_all(q_table, "<span>[0-9]*\\.?[0-9]</span>"))
str_replace_all(flows, "</?span>")
str_replace_all(flows, "</?span>", "")
str_replace_all(flows, "<span>", "")
flows
flows = unlist(str_match_all(q_table, "<span>[0-9]*\\.?[0-9]</span>"))
str_replace_all(flows, "<span>", "")
str_replace_all(flows, "</?span>", "")
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '^<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = unlist(str_match_all(q_table, "<span>[0-9]*\\.?[0-9]</span>"))
flows = as.numeric(str_replace_all(flows, "</?span>", ""))
flows
q_table[1:10]
q_table[15:30]
q_table[21:30]
dates
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = unlist(str_match_all(q_table, "<span>[0-9]*\\.?[0-9]</span>"))
flows = as.numeric(str_replace_all(flows, "</?span>", ""))
flows
unlist(str_match_all(q_table, "<span>[0-9]*\\.?[0-9]</span>"))
unlist(str_match_all(q_table, "<span>.*</span>"))
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = unlist(str_match_all(q_table, "<span>.*</span>"))
flows = as.numeric(str_replace_all(flows, "</?span>", ""))
flows
# Specify the inputs: gauge number, q_code is fixed, start date and end date
site_no <- '08324000'
q_code <- '00060'
start <- '2000-01-01'
end <- '2020-12-31'
# Define function
get_discharge_data <- function(site_no, q_code, start, end) {
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%y")
flows = unlist(str_match_all(q_table, "<span>.*</span>"))
flows = as.numeric(str_replace_all(flows, "</?span>", ""))
# Put dates and flow together as a matrix
Q_df = data.frame(
Dates = dates,
Qobs = flows
)
return(Q_df)
}
q_df <- get_discharge_data(site_no, q_code, start, end)
library(ggplot2)
validate_q <- readNWISdv(siteNumber = site_no, parameterCd = q_code,
start, end)
# install.packages("dataRetrieval")
library(dataRetrieval)
suppressMessages(library(tidyverse))
library(dataRetrieval)
library(dplyr)
library(ggplot2)
update()
updateR()
install.packages("updater")
# install.packages("dataRetrieval")
library(dataRetrieval)
suppressMessages(library(tidyverse))
library(dataRetrieval)
library(dplyr)
library(ggplot2)
updater
sessioninfo()
# install.packages("dataRetrieval")
library(dataRetrieval)
suppressMessages(library(tidyverse))
library(dataRetrieval)
library(dplyr)
library(ggplot2)
# Define function to get discharge timeseries
get_discharge_data <- function(site_no, q_code, start, end) {
# Build the url link
url <- paste0('https://waterdata.usgs.gov/nwis/dv?cb_',
q_code,
'=on&format=html&site_no=',
site_no,
'&legacy=&re[…]module=sw&period=&begin_date=',
start,
'&end_date=',
end)
q_data <- readLines(url)
# Select lines which contains the data
table_pattern <- '<tr align="center"><td nowrap="nowrap">'
q_table <- str_subset(q_data, table_pattern)
# Extract dates and flow(ft3/s) from each line
dates = as.Date(unlist(str_match_all(q_table, "[0-9]*/[0-9]*/[0-9]*")), "%m/%d/%Y")
flows = unlist(str_match_all(q_table, "<span>.*</span>"))
flows = as.numeric(str_replace_all(flows, "</?span>", ""))
# Put dates and flow together as a matrix
Q_df = data.frame(
Date = dates,
Qobs = flows
)
return(Q_df)
}
get_drainage_area = function(site_no){
# Scrape the drainage area
url2 <- paste0('https://waterdata.usgs.gov/nwis/inventory/?site_no=',
site_no,
'&agency_cd=USGS')
d_data <- readLines(url2)
d_pattern <- "^.*Drainage area"
d_line <- str_subset(d_data,d_pattern)
darea <- unique(unlist(str_extract_all(d_line, "[0-9]*\\.?[0-9]*")))[2]
darea <- as.numeric(darea)
return(darea)
}
# Specify the inputs: gauge number, q_code is fixed, start date and end date
site_no <- '08324000'
q_code <- '00060'
start <- '2000-01-01'
end <- '2020-12-31'
# Use the function we have to scrape
q_df <- get_discharge_data(site_no, q_code, start, end)
# Get authentic streamflow data using the package
validate_q <- readNWISdv(siteNumber = site_no, parameterCd = q_code,
start, end)
# Calculate the error between scraped data and the true data
scrape_error = q_df$Qobs - validate_q$X_00060_00003
plot(q_df$Date, scrape_error)
# Note: not all the site have a validate
gauges_area = read.csv("drainage_area.txt", colClasses = c("character", "numeric"))
# Note: not all the site have a validate
gauges_area = read.csv("drainage_area.txt", colClasses = c("character", "numeric"))
head(gauges_area)
quantile(x = q_df$Qobs, probs = c(.05, .5, .95))
gauges_area
# Note: not all the site have a validate
# 0 drainage area meant no data avaliable
# Filter gauges which do not have the drainage area data
gauges_area = read.csv("drainage_area.txt", colClasses = c("character", "numeric"))
gauges_area = gauges_area %%
filter(drainage_area > 0)
gauges_area
# Note: not all the site have a validate
# 0 drainage area meant no data avaliable
# Filter gauges which do not have the drainage area data
gauges_area = read.csv("drainage_area.txt", colClasses = c("character", "numeric"))
gauges_area = gauges_area %%
filter(drainage_area > 0)
# Note: not all the site have a validate
# 0 drainage area meant no data avaliable
# Filter gauges which do not have the drainage area data
gauges_area = read.csv("drainage_area.txt", colClasses = c("character", "numeric"))
gauges_area = gauges_area %>%
filter(drainage_area > 0)
gauges_area
row(gauges_area)
length(gauges_area)
flow_df <- read.csv(paste0("streamflows/",
site_no, ".csv")
)
setwd("C:/Users/xinch/OneDrive - University of Massachusetts/2022Fall_Courses/Statistical_computing/FP/STAT535")
for (site_no in site_list){
flow_df <- read.csv(paste0("streamflows/",
site_no, ".csv")
)
)
flow_df <- read.csv(paste0("streamflows/",
site_no, ".csv")
)
flow_df <- read.csv(paste0("streamflows/",
01333000, ".csv")
)
flow_df <- read.csv(paste0("streamflows/",01333000,".csv"), )
flow_df <- read.csv(paste0("streamflows/",'01333000',".csv"), )
flow_df
flow_df$Qobs
Q5 = quantile(x = flow_df$Qobs, probs = c(.95))
Q5
quantile(x = flow_df$Qobs, probs = c(.95), na.rm = TRUE)
quantile(x = flow_df$Qobs, probs = c(.95), na.rm = TRUE)[0]
quantile(x = flow_df$Qobs, probs = c(.95), na.rm = TRUE)[1]
uname(quantile(x = flow_df$Qobs, probs = c(.95), na.rm = TRUE))
unname(quantile(x = flow_df$Qobs, probs = c(.95), na.rm = TRUE))
# Note: not all the gauges have enough streamflow data
site_list <- character()
Q5s <- numeric()
Q50s <- numeric()
Q95s <- numeric()
for (site_no in gauges_area$site_no){
flow_df <- read.csv(paste0("streamflows/",
site_no, ".csv")
)
# check how many data in the data
if (nrow(flow_df) <= 730){
# if less 2 years data were scraped
# go to next gauge
print(paste0("Less than 2 years in the site:", site_no))
}else{
# more than 2 years data were scraped
# the gauge would be saved into the site_list
# save the site no
site_list = c(site_list, site_no)
# Calculate the statistics
Q5 = unname(quantile(x = flow_df$Qobs, probs = c(.95), na.rm = TRUE))
Q50 = unname(quantile(x = flow_df$Qobs, probs = c(.5), na.rm = TRUE))
Q95 = unname(quantile(x = flow_df$Qobs, probs = c(.05), na.rm = TRUE))
# save the statistics
Q5s = c(Q5s, Q5)
Q50s = c(Q50s, Q50)
Q95s = c(Q95s, Q95)
}
}
# The dataframe is based on the gauge area dataframe
Q_A_df = gauges_area[gauges_area$site_no = site_no]
# The dataframe is based on the gauge area dataframe
Q_A_df = gauges_area[site_no = site_list]
# The dataframe is based on the gauge area dataframe
Q_A_df = gauges_area %>%
filter(site_no %in% site_list)
Q_A_df
# The dataframe is based on the gauge area dataframe
Q_A_df = gauges_area %>%
filter(site_no %in% site_list) %>%
mutate(Q5 = Q5s) %>%
mutate(Q50 = Q50s) %>%
mutate(Q95 = Q95s)
Q_A_df
